{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train = pd.read_csv('qqp-train.csv', dtype=str)\n",
    "qqp_test = pd.read_csv('qqp-test.csv', dtype=str)\n",
    "\n",
    "sts_train = pd.read_csv('sts-train.csv', dtype=str)\n",
    "sts_test = pd.read_csv('sts-test.csv', dtype=str)\n",
    "\n",
    "msr_train = pd.read_csv('msr-train.csv', sep='\\t')\n",
    "msr_test = pd.read_csv('msr-test.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train['similar'] = qqp_train['similar'].astype(str)\n",
    "qqp_test['similar'] = qqp_test['similar'].astype(str)\n",
    "\n",
    "qqp_train['Sentence1'] = qqp_train['Sentence1'].astype(str)\n",
    "qqp_test['Sentence1'] = qqp_test['Sentence1'].astype(str)\n",
    "\n",
    "qqp_train['Sentence2'] = qqp_train['Sentence2'].astype(str)\n",
    "qqp_test['Sentence2'] = qqp_test['Sentence2'].astype(str)\n",
    "\n",
    "qqp = pd.concat([qqp_train, qqp_test])\n",
    "\n",
    "#########################################################\n",
    "\n",
    "sts_train['Sentence1'] = sts_train['Sentence1'].astype(str)\n",
    "sts_train['Sentence2'] = sts_train['Sentence2'].astype(str)\n",
    "sts_train['similar'] = sts_train['similar'].astype(str)\n",
    "\n",
    "\n",
    "sts_test['Sentence1'] = sts_test['Sentence1'].astype(str)\n",
    "sts_test['Sentence2'] = sts_test['Sentence2'].astype(str)\n",
    "sts_test['similar'] = sts_test['similar'].astype(str)\n",
    "\n",
    "sts = pd.concat([sts_train, sts_test])\n",
    "\n",
    "#########################################################\n",
    "\n",
    "msr_train = msr_train.rename(columns={'#1 String': 'Sentence1', '#2 String': 'Sentence2', 'Quality': 'similar'})\n",
    "msr_test = msr_test.rename(columns={'#1 String': 'Sentence1', '#2 String': 'Sentence2', 'Quality': 'similar'})\n",
    "msr_train = msr_train.drop('Unnamed: 5', axis=1)\n",
    "msr_test = msr_test.drop('Unnamed: 5', axis=1)\n",
    "msr_train = msr_train.dropna()\n",
    "msr_test = msr_test.dropna()\n",
    "msr_train = msr_train[~msr_train['Sentence2'].str.contains('\\t')]\n",
    "msr_test = msr_test[~msr_test['Sentence1'].str.contains('\\t')]\n",
    "\n",
    "msr = pd.concat([msr_train, msr_test])\n",
    "\n",
    "# our stop words:\n",
    "stop_words = set([\n",
    "    \"the\", \"a\", \"an\", \"of\", \"to\", \"in\", \"for\", \"with\", \"on\", \"at\", \"from\", \"by\",\"also\", \"as\",\"so\", \"and\", \"but\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, beautiful_case = True):\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    total_common_count=0\n",
    "    total_noncommon_count=0\n",
    "    \n",
    "    average = 0.0\n",
    "    n = 0.0\n",
    "    for i, row in df.iterrows():\n",
    "        S1, S2 = row['Sentence1'], row['Sentence2']\n",
    "        S1 = S1.lower()\n",
    "        S2 = S2.lower()\n",
    "        if beautiful_case:\n",
    "            S1 = re.sub(r'[^\\w\\s]', '', S1)\n",
    "            S2 = re.sub(r'[^\\w\\s]', '', S2)\n",
    "            \n",
    "            S1_tokens = [word for word in S1.split() if word not in stop_words]\n",
    "            S2_tokens = [word for word in S2.split() if word not in stop_words]\n",
    "\n",
    "        else: \n",
    "            S1_tokens = S1.split()\n",
    "            S2_tokens = S2.split()\n",
    "        \n",
    "        counter1 = Counter(S1_tokens)\n",
    "        counter2 = Counter(S2_tokens)\n",
    "\n",
    "        # Find common elements and their counts\n",
    "        common_elements_count = (counter1 & counter2).items()\n",
    "        unique_to_list1 = counter1 - counter2\n",
    "        unique_to_list2 = counter2 - counter1\n",
    "\n",
    "        # Combine the results to mimic the symmetric difference\n",
    "        unique_elements = unique_to_list1 + unique_to_list2\n",
    "\n",
    "        non_common = sum(unique_elements.values())\n",
    "        total_noncommon_count += non_common\n",
    "        # Calculate the total number of common elements\n",
    "        total_common = sum(min(counter1[element], counter2[element]) for element in (counter1 & counter2))\n",
    "        total_common_count += total_common\n",
    "\n",
    "        if total_common + non_common == 0:\n",
    "            print(S1, S2)\n",
    "\n",
    "        average += total_common / (total_common + non_common)\n",
    "        \n",
    "        if average > .8:\n",
    "            pass\n",
    "            #print(S1, S2)\n",
    "\n",
    "        n+= 1\n",
    "\n",
    "    return total_common_count, total_noncommon_count, average/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_extractor(word):\n",
    "    synonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def synonym_counter(words1, words2):\n",
    "    # words1 = sentence1.split()\n",
    "    # words2 = sentence2.split()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for word1 in words1:\n",
    "        synonyms = synonym_extractor(word1)\n",
    "        for word2 in words2:\n",
    "            if word2 in synonyms:\n",
    "                count += 1\n",
    "                \n",
    "    # we do it again because the synonym relation is not symmetric\n",
    "    for word2 in words2:\n",
    "        synonyms = synonym_extractor(word2)\n",
    "        for word1 in words1:\n",
    "            if word1 in synonyms:\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def preprocessing_synonym_counter(df, beautiful_case = True):\n",
    "    total_similar_count=0\n",
    "    total_nonsimilar_count=0\n",
    "    \n",
    "    expected_porcentage = 0.0\n",
    "\n",
    "    sum = 0.0\n",
    "\n",
    "    n = 0\n",
    "    for i, row in df.iterrows():\n",
    "        S1, S2 = row['Sentence1'], row['Sentence2']\n",
    "        S1 = S1.lower()\n",
    "        S2 = S2.lower()\n",
    "        if beautiful_case:\n",
    "            S1 = re.sub(r'[^\\w\\s]', '', S1)\n",
    "            S2 = re.sub(r'[^\\w\\s]', '', S2)\n",
    "            \n",
    "            S1_tokens = [word for word in S1.split() if word not in stop_words]\n",
    "            S2_tokens = [word for word in S2.split() if word not in stop_words]\n",
    "\n",
    "        else: \n",
    "            S1_tokens = S1.split()\n",
    "            S2_tokens = S2.split()\n",
    "\n",
    "\n",
    "        similar_count = synonym_counter(S1_tokens, S2_tokens)\n",
    "        total_nonsimilar_count += len(S1_tokens) + len(S2_tokens) - synonym_counter(S1_tokens, S2_tokens)\n",
    "\n",
    "        expected_porcentage = similar_count / (len(S1_tokens) + len(S2_tokens))\n",
    "\n",
    "        total_similar_count += similar_count\n",
    "        \n",
    "        sum += expected_porcentage\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "    return total_similar_count, total_nonsimilar_count, sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_tfidf(s1,s2):\n",
    "    # Combine the sentences into one list for vectorization\n",
    "    all_sentences = s1 + s2\n",
    "\n",
    "    # Initialize a TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    #cos_sim = cosine_similarity([vec_sentence1], [vec_sentence2])[0][0]\n",
    "\n",
    "    # Define a threshold\n",
    "    threshold = 0.5\n",
    "    # Generate embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(all_sentences)\n",
    "\n",
    "    # Now, each pair of sentences at the same index can be compared\n",
    "    similarities_tf = []\n",
    "    similarities_sbert = []\n",
    "    for i in range(len(s1)):\n",
    "        # Compute cosine similarity between corresponding sentence pairs for TF-IDF\n",
    "        sim_score_tf = cosine_similarity(tfidf_matrix[i], tfidf_matrix[len(s1) + i])[0][0]\n",
    "        \n",
    "        # Compute cosine similarity between corresponding sentence pairs for SBERT\n",
    "        sim_score_sbert = cosine_similarity(\n",
    "            embeddings[i].reshape(1, -1),  # Reshape embeddings to 2D\n",
    "            embeddings[len(s1) + i].reshape(1, -1)  # Reshape embeddings to 2D\n",
    "        )[0][0]\n",
    "\n",
    "        # Append the binarized similarity scores\n",
    "        similarities_tf.append(1 if sim_score_tf >= threshold else 0)\n",
    "        similarities_sbert.append(1 if sim_score_sbert >= threshold else 0)\n",
    "\n",
    "    return similarities_tf, similarities_sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQP Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Percentage for QQP: 0.35777842667844834\n",
      "Common Words Percentage for QQP: 0.38048851593150695\n"
     ]
    }
   ],
   "source": [
    "print(f'Synonym Percentage for QQP: {preprocessing_synonym_counter(qqp)[2]}')\n",
    "print(f'Common Words Percentage for QQP: {preprocessing(qqp)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STS Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Percentage for STS: 0.3515383600414239\n",
      "Common Words Percentage for STS: 0.3683107783596161\n"
     ]
    }
   ],
   "source": [
    "print(f'Synonym Percentage for STS: {preprocessing_synonym_counter(sts)[2]}')\n",
    "print(f'Common Words Percentage for STS: {preprocessing(sts)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSR Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Percentage for MSR: 0.42952693053548047\n",
      "Common Words Percentage for MSR: 0.4839155134597848\n"
     ]
    }
   ],
   "source": [
    "print(f'Synonym Percentage for MSR: {preprocessing_synonym_counter(msr)[2]}')\n",
    "print(f'Common Words Percentage for MSR: {preprocessing(msr)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without removing anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQP Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Percentage for QQP: 0.26190876537563695\n",
      "Common Words Percentage for QQP: 0.34165718184541033\n"
     ]
    }
   ],
   "source": [
    "print(f'Synonym Percentage for QQP: {preprocessing_synonym_counter(qqp, beautiful_case=False)[2]}')\n",
    "print(f'Common Words Percentage for QQP: {preprocessing(qqp, beautiful_case=False)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STS Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Percentage for STS: 0.3590200797328076\n",
      "Common Words Percentage for STS: 0.36522429674991985\n"
     ]
    }
   ],
   "source": [
    "print(f'Synonym Percentage for STS: {preprocessing_synonym_counter(sts, beautiful_case=False)[2]}')\n",
    "print(f'Common Words Percentage for STS: {preprocessing(sts, beautiful_case=False)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSR Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Percentage for MSR: 0.32384588813020865\n",
      "Common Words Percentage for MSR: 0.4418652557801848\n"
     ]
    }
   ],
   "source": [
    "print(f'Synonym Percentage for MSR: {preprocessing_synonym_counter(msr, beautiful_case=False)[2]}')\n",
    "print(f'Common Words Percentage for MSR: {preprocessing(msr, beautiful_case=False)[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
