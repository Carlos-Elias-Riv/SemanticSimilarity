{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train = pd.read_csv('qqp-train.tsv', sep='\\t', dtype=str)\n",
    "qqp_test = pd.read_csv('qqp-test.tsv', sep='\\t', dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train = qqp_train.rename(columns={'question1': 'Sentence1', 'question2': 'Sentence2', 'is_duplicate': 'similar'})\n",
    "qqp_test = qqp_test.rename(columns={'question1': 'Sentence1', 'question2': 'Sentence2', 'is_duplicate': 'similar'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17145/90640678.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  qqp_train = qqp_train.groupby('similar', group_keys=False).apply(lambda x: x.sample(2500))\n",
      "/tmp/ipykernel_17145/90640678.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  qqp_test = qqp_test.groupby('similar', group_keys=False).apply(lambda x: x.sample(500))\n"
     ]
    }
   ],
   "source": [
    "qqp_train = qqp_train.groupby('similar', group_keys=False).apply(lambda x: x.sample(2500))\n",
    "qqp_test = qqp_test.groupby('similar', group_keys=False).apply(lambda x: x.sample(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train = qqp_train.sample(frac=1)\n",
    "qqp_test = qqp_test.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train = qqp_train.drop_duplicates()\n",
    "qqp_test = qqp_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train_test_merge = qqp_train.merge(qqp_test, how='inner', indicator=True)\n",
    "qqp_test = qqp_test.drop(qqp_train_test_merge.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqp_train['similar'] = qqp_train['similar'].astype(str)\n",
    "qqp_test['similar'] = qqp_test['similar'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    total_common_count=0\n",
    "    total_noncommon_count=0\n",
    "    sentence1_array=[]\n",
    "    sentence2_array=[]\n",
    "    target_output=[]\n",
    "    average = 0.0\n",
    "    n = 0.0\n",
    "    for i, row in df.iterrows():\n",
    "        S1, S2 = row['Sentence1'], row['Sentence2']\n",
    "        \n",
    "        sentence1_array.append(S1)\n",
    "        sentence2_array.append(S2)\n",
    "        target_output.append(int(row['similar']))\n",
    "        \n",
    "        #punctuations = [char for char in S1 if char in string.punctuation]\n",
    "        # print(punctuations)\n",
    "        #punct.extend(punctuations)\n",
    "        #S1 = re.sub()\n",
    "\n",
    "        S1.lower()\n",
    "        S2.lower()\n",
    "        \n",
    "        S1_tokens = S1.split()\n",
    "        S2_tokens = S2.split()\n",
    "\n",
    "        counter1 = Counter(S1_tokens)\n",
    "        counter2 = Counter(S2_tokens)\n",
    "\n",
    "        # Find common elements and their counts\n",
    "        common_elements_count = (counter1 & counter2).items()\n",
    "        unique_to_list1 = counter1 - counter2\n",
    "        unique_to_list2 = counter2 - counter1\n",
    "\n",
    "        # Combine the results to mimic the symmetric difference\n",
    "        unique_elements = unique_to_list1 + unique_to_list2\n",
    "\n",
    "        non_common = sum(unique_elements.values())\n",
    "        total_noncommon_count += non_common\n",
    "        # Calculate the total number of common elements\n",
    "        total_common = sum(min(counter1[element], counter2[element]) for element in (counter1 & counter2))\n",
    "        total_common_count += total_common\n",
    "\n",
    "        average += total_common / (total_common + non_common)\n",
    "\n",
    "        n+= 1\n",
    "\n",
    "    return total_common_count, total_noncommon_count, average/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_extractor(word):\n",
    "    synonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def synonym_counter(sentence1, sentence2):\n",
    "    words1 = sentence1.split()\n",
    "    words2 = sentence2.split()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for word1 in words1:\n",
    "        synonyms = synonym_extractor(word1)\n",
    "        for word2 in words2:\n",
    "            if word2 in synonyms:\n",
    "                count += 1\n",
    "                \n",
    "    # we do it again because the synonym relation is not symmetric\n",
    "    for word2 in words2:\n",
    "        synonyms = synonym_extractor(word2)\n",
    "        for word1 in words1:\n",
    "            if word1 in synonyms:\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def preprocessing_synonym_counter(df):\n",
    "    total_similar_count=0\n",
    "    total_nonsimilar_count=0\n",
    "    \n",
    "    expected_porcentage = 0.0\n",
    "\n",
    "    sum = 0.0\n",
    "\n",
    "    n = 0\n",
    "    for i, row in df.iterrows():\n",
    "        S1, S2 = row['Sentence1'], row['Sentence2']\n",
    "\n",
    "        similar_count = synonym_counter(S1, S2)\n",
    "        total_nonsimilar_count += len(S1.split()) + len(S2.split()) - synonym_counter(S1, S2)\n",
    "\n",
    "        expected_porcentage = similar_count / (len(S1.split()) + len(S2.split()))\n",
    "\n",
    "        total_similar_count += similar_count\n",
    "        \n",
    "        sum += expected_porcentage\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "    return total_similar_count, total_nonsimilar_count, sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_tfidf(s1,s2):\n",
    "    # Combine the sentences into one list for vectorization\n",
    "    all_sentences = s1 + s2\n",
    "\n",
    "    # Initialize a TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    #cos_sim = cosine_similarity([vec_sentence1], [vec_sentence2])[0][0]\n",
    "\n",
    "    # Define a threshold\n",
    "    threshold = 0.5\n",
    "    # Generate embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(all_sentences)\n",
    "\n",
    "    # Now, each pair of sentences at the same index can be compared\n",
    "    similarities_tf = []\n",
    "    similarities_sbert = []\n",
    "    for i in range(len(s1)):\n",
    "        # Compute cosine similarity between corresponding sentence pairs for TF-IDF\n",
    "        sim_score_tf = cosine_similarity(tfidf_matrix[i], tfidf_matrix[len(s1) + i])[0][0]\n",
    "        \n",
    "        # Compute cosine similarity between corresponding sentence pairs for SBERT\n",
    "        sim_score_sbert = cosine_similarity(\n",
    "            embeddings[i].reshape(1, -1),  # Reshape embeddings to 2D\n",
    "            embeddings[len(s1) + i].reshape(1, -1)  # Reshape embeddings to 2D\n",
    "        )[0][0]\n",
    "\n",
    "        # Append the binarized similarity scores\n",
    "        similarities_tf.append(1 if sim_score_tf >= threshold else 0)\n",
    "        similarities_sbert.append(1 if sim_score_sbert >= threshold else 0)\n",
    "\n",
    "    return similarities_tf, similarities_sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27231, 80478, 0.2493202622831785)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_synonym_counter(qqp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22814, 62081, 0.3238331113106615)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(qqp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
